{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f27cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cuffless Blood Pressure Estimation using CNN-BiLSTM-Attention Model\n",
    "Generates regression + Bland–Altman plots\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Dense, Dropout, Bidirectional, LSTM, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ------------------ ATTENTION LAYER ------------------\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weights',\n",
    "                                shape=(input_shape[-1], self.units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                shape=(self.units,),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "        self.u = self.add_weight(name='attention_context',\n",
    "                                shape=(self.units,),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        uit = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        ait = tf.nn.softmax(ait, axis=1)\n",
    "        ait = tf.expand_dims(ait, axis=-1)\n",
    "        weighted_input = inputs * ait\n",
    "        output = tf.reduce_sum(weighted_input, axis=1)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "# ------------------ PREPROCESSING ------------------\n",
    "def preprocess_ppg_signal(ppg_signal, sampling_rate=125):\n",
    "    time_axis = np.arange(len(ppg_signal))\n",
    "    coeffs = np.polyfit(time_axis, ppg_signal, 1)\n",
    "    trend = np.polyval(coeffs, time_axis)\n",
    "    detrended_signal = ppg_signal - trend\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    normalized_signal = scaler.fit_transform(detrended_signal.reshape(-1, 1)).flatten()\n",
    "    return normalized_signal\n",
    "\n",
    "def extract_windows(ppg_signal, abp_signal, window_size=8.192, overlap=0.75, sampling_rate=125):\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    step_samples = int(window_samples * (1 - overlap))\n",
    "    ppg_windows, sbp_targets, dbp_targets = [], [], []\n",
    "\n",
    "    for start_idx in range(0, len(ppg_signal) - window_samples + 1, step_samples):\n",
    "        end_idx = start_idx + window_samples\n",
    "        ppg_window = ppg_signal[start_idx:end_idx]\n",
    "        abp_window = abp_signal[start_idx:end_idx]\n",
    "\n",
    "        peaks, _ = signal.find_peaks(abp_window, height=np.mean(abp_window), distance=int(0.6*sampling_rate))\n",
    "        troughs, _ = signal.find_peaks(-abp_window, height=-np.mean(abp_window), distance=int(0.6*sampling_rate))\n",
    "\n",
    "        if len(peaks) > 0 and len(troughs) > 0:\n",
    "            sbp = np.mean(abp_window[peaks])\n",
    "            dbp = np.mean(abp_window[troughs])\n",
    "        else:\n",
    "            sbp = np.max(abp_window)\n",
    "            dbp = np.min(abp_window)\n",
    "\n",
    "        ppg_windows.append(ppg_window)\n",
    "        sbp_targets.append(sbp)\n",
    "        dbp_targets.append(dbp)\n",
    "\n",
    "    return np.array(ppg_windows), np.array(sbp_targets), np.array(dbp_targets)\n",
    "\n",
    "def load_patient_data(npz_file_path):\n",
    "    try:\n",
    "        data = np.load(npz_file_path)\n",
    "        return data['ppg'], data['abp']\n",
    "    except:\n",
    "        print(f\"Error loading {npz_file_path}\")\n",
    "        return None, None\n",
    "\n",
    "def load_dataset(data_folder_path):\n",
    "    npz_files = glob.glob(os.path.join(data_folder_path, \"*.npz\"))\n",
    "    print(f\"Found {len(npz_files)} NPZ files\")\n",
    "    all_ppg_windows, all_sbp_targets, all_dbp_targets = [], [], []\n",
    "\n",
    "    for i, npz_file in enumerate(npz_files):\n",
    "        ppg, abp = load_patient_data(npz_file)\n",
    "        if ppg is None or abp is None or len(ppg) < 1024:\n",
    "            continue\n",
    "        ppg_processed = preprocess_ppg_signal(ppg)\n",
    "        ppg_windows, sbp_targets, dbp_targets = extract_windows(ppg_processed, abp)\n",
    "        all_ppg_windows.extend(ppg_windows)\n",
    "        all_sbp_targets.extend(sbp_targets)\n",
    "        all_dbp_targets.extend(dbp_targets)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(npz_files)} patients\")\n",
    "\n",
    "    X = np.array(all_ppg_windows).reshape(-1, 1024, 1)\n",
    "    y_sbp = np.array(all_sbp_targets)\n",
    "    y_dbp = np.array(all_dbp_targets)\n",
    "    print(f\"Loaded dataset shape: {X.shape}\")\n",
    "    return X, y_sbp, y_dbp\n",
    "\n",
    "# ------------------ MODEL ------------------\n",
    "def create_cnn_bilstm_attention_model(input_shape=(1024, 1)):\n",
    "    inputs = Input(shape=input_shape, name='ppg_input')\n",
    "    \n",
    "    # CNN layers for feature extraction (3 layers as per paper)\n",
    "    # First CNN layer\n",
    "    x = Conv1D(filters=32, kernel_size=3, strides=1, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = MaxPool1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Second CNN layer  \n",
    "    x = Conv1D(filters=64, kernel_size=3, strides=1, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = MaxPool1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Third CNN layer\n",
    "    x = Conv1D(filters=128, kernel_size=3, strides=1, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = MaxPool1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # BiLSTM layers (2 layers as per paper)\n",
    "    # First BiLSTM layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Second BiLSTM layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Attention mechanism\n",
    "    attention_output = AttentionLayer(units=128)(x)\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    x = Dense(64, activation='relu')(attention_output)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layers for SBP and DBP\n",
    "    sbp_output = Dense(1, activation='linear', name='sbp_output')(x)\n",
    "    dbp_output = Dense(1, activation='linear', name='dbp_output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=[sbp_output, dbp_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ------------------ PLOTS ------------------\n",
    "def plot_regression(y_true, y_pred, label):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, s=10, alpha=0.5)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')\n",
    "    plt.xlabel(f\"True {label} (mmHg)\")\n",
    "    plt.ylabel(f\"Predicted {label} (mmHg)\")\n",
    "    plt.title(f\"{label} Regression Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_bland_altman(y_true, y_pred, label):\n",
    "    mean_vals = (y_true + y_pred) / 2\n",
    "    diff = y_true - y_pred\n",
    "    md = np.mean(diff)\n",
    "    sd = np.std(diff)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(mean_vals, diff, s=10, alpha=0.5)\n",
    "    plt.axhline(md, color='r', linestyle='--', label='Mean Diff')\n",
    "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--', label='±1.96 SD')\n",
    "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.xlabel(f\"Mean {label} (mmHg)\")\n",
    "    plt.ylabel(f\"Difference (True - Pred)\")\n",
    "    plt.title(f\"{label} Bland–Altman Plot\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ------------------ MAIN ------------------\n",
    "def main():\n",
    "    data_folder = r\"\\preprocessed_patients\"\n",
    "    X, y_sbp, y_dbp = load_dataset(data_folder)\n",
    "\n",
    "    # 1-fold split: 80% train, 20% test\n",
    "    X_train, X_test, y_sbp_train, y_sbp_test, y_dbp_train, y_dbp_test = train_test_split(\n",
    "        X, y_sbp, y_dbp, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "    model = create_cnn_bilstm_attention_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-3),\n",
    "        loss={'sbp_output': 'mse', 'dbp_output': 'mse'},\n",
    "        metrics={'sbp_output': 'mae', 'dbp_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6),\n",
    "        ModelCheckpoint(\"best_model_fold1.h5\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {'sbp_output': y_sbp_train, 'dbp_output': y_dbp_train},\n",
    "        validation_split=0.1,\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Load best model\n",
    "    model = tf.keras.models.load_model(\"best_model_fold1.h5\", custom_objects={'AttentionLayer': AttentionLayer})\n",
    "    sbp_pred, dbp_pred = model.predict(X_test)\n",
    "\n",
    "    sbp_mae = mean_absolute_error(y_sbp_test, sbp_pred)\n",
    "    dbp_mae = mean_absolute_error(y_dbp_test, dbp_pred)\n",
    "    print(f\"SBP MAE: {sbp_mae:.2f}, DBP MAE: {dbp_mae:.2f}\")\n",
    "\n",
    "    # --- Regression and Bland–Altman plots ---\n",
    "    plot_regression(y_sbp_test, sbp_pred, \"SBP\")\n",
    "    plot_bland_altman(y_sbp_test, sbp_pred, \"SBP\")\n",
    "\n",
    "    plot_regression(y_dbp_test, dbp_pred, \"DBP\")\n",
    "    plot_bland_altman(y_dbp_test, dbp_pred, \"DBP\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
